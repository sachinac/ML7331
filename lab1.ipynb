{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Sachin, Ikenna, Edgar, Dustin\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "Our selected dataset is a labeled collection of inpatient medical claims, non-personally identifiable beneficiary data, and a list of providers who have been suspected as filing fraudulent claims.  Detecting fraudulent claims is a request frequently asked by a great many organizations in the healthcare industry (and especially in insurance), and claim data is readily available at many of these organizations, so it stands to reason that we may be able to predict fraudulent behavior by analyzing a large number of medical claims.  We will analyze over 40,000 medical claims, non-personally identifying beneficiary information and provider IDs who have previously been identified as engaging in suspected fraudulent behavior in order to attempt to predict possible fraudulent behavior in new claims.\n",
    "\n",
    "Because we have a categorical response variable, we'll need to treat this as a classification problem.  We will predict possible fraudulent claims and measure effectiveness using cross validation and focus on attaining high metrics in precision and recall.  Precision measures the percentage of fraudulent predictions which are truly fraudulent, and recall measures the total percentage of fraudulent claims correctly identified.  These two metrics have been identified as most appropriate, due to our objective of correctly identifying fraudulent claims.\n",
    "\n",
    "Some potential outcomes of using this dataset are potentially improving viability of existing provider fraud detection mechanisms in insurance and member paid claims.  Knowledge of fraudulent or excessive billed services awareness for what is or is not acceptable can save money and help reduce rising healthcare costs due to rising insurance premiums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "#Read in data:\n",
    "ipdata = pd.read_csv(\"./data/Train_Inpatientdata-1542865627584.csv\")\n",
    "bendata = pd.read_csv(\"./data/Train_Beneficiarydata-1542865627584.csv\")\n",
    "prodata = pd.read_csv(\"./data/Train-1542865627584.csv\")\n",
    "\n",
    "\n",
    "proc_codes = pd.read_csv(\"./data/CMS27_DESC_LONG_SHORT_SG_092709.csv\")\n",
    "# v27 ICD-9 Procedure Code Data comes from https://www.cms.gov/Medicare/Coding/ICD9ProviderDiagnosticCodes/Downloads/FY2010Diagnosis-ProcedureCodesFullTitles.zip\n",
    "\n",
    "diag_codes = pd.read_fwf(\"./data/V26 I-9 Diagnosis.txt\")\n",
    "# v26 Diag Code Data comes from https://www.cms.gov/Medicare/Coding/ICD9ProviderDiagnosticCodes/Downloads/v27_icd9.zip\n",
    "# and missing code 7889 comes from https://www.cms.gov/Medicare/Coding/ICD9ProviderDiagnosticCodes/Downloads/v26_icd9.zip\n",
    "\n",
    "\n",
    "\n",
    "# Merge beneficiary data with inpatient data:\n",
    "ipdata = ipdata.merge(bendata, left_on='BeneID', right_on='BeneID')\n",
    "\n",
    "# Merge Provider Data:\n",
    "ipdata = ipdata.merge(prodata, on='Provider', how='inner')\n",
    "\n",
    "# Fill NAs\n",
    "ipdata['DOD'] = ipdata['DOD'].fillna('2199-12-31')\n",
    "missing_vcols = ['OperatingPhysician', 'OtherPhysician','ClmAdmitDiagnosisCode','DiagnosisGroupCode', \n",
    "                 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', \n",
    "                 'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7','ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', \n",
    "                 'ClmDiagnosisCode_10','ClmProcedureCode_1', 'ClmProcedureCode_2', 'ClmProcedureCode_3', \n",
    "                 'ClmProcedureCode_4', 'ClmProcedureCode_5', 'ClmProcedureCode_6'] \n",
    "\n",
    "ipdata[missing_vcols] = ipdata[missing_vcols].fillna('None')\n",
    "ipdata['DeductibleAmtPaid'] = ipdata['DeductibleAmtPaid'].fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Convert everythign to string\n",
    "ipdata = ipdata.applymap(str)\n",
    "\n",
    "# Convert to usable formats:\n",
    "timecols = ['ClaimStartDt', 'ClaimEndDt','AdmissionDt', 'DischargeDt', 'DOB','DOD']\n",
    "numcols = ['InscClaimAmtReimbursed', 'DeductibleAmtPaid','IPAnnualReimbursementAmt', 'IPAnnualDeductibleAmt']\n",
    "catcols = ['ChronicCond_Alzheimer','ChronicCond_Heartfailure','ChronicCond_KidneyDisease','ChronicCond_Cancer','ChronicCond_ObstrPulmonary','ChronicCond_Depression','ChronicCond_Diabetes','ChronicCond_IschemicHeart','ChronicCond_Osteoporasis','ChronicCond_rheumatoidarthritis','ChronicCond_stroke','PotentialFraud']\n",
    "\n",
    "ipdata[timecols] = ipdata[timecols].apply(pd.to_datetime)\n",
    "\n",
    "ipdata['DOD'] = ipdata['DOD'].apply(dt.datetime.toordinal)\n",
    "ipdata['DOB'] = ipdata['DOB'].apply(dt.datetime.toordinal)\n",
    "ipdata['ClaimStartDt'] = ipdata['ClaimStartDt'].apply(dt.datetime.toordinal)\n",
    "ipdata['ClaimEndDt'] = ipdata['ClaimEndDt'].apply(dt.datetime.toordinal)\n",
    "ipdata['AdmissionDt'] = ipdata['AdmissionDt'].apply(dt.datetime.toordinal)\n",
    "ipdata['DischargeDt'] = ipdata['DischargeDt'].apply(dt.datetime.toordinal)\n",
    "\n",
    "#ipdata[timecols] = ipdata[timecols].apply(pd.to_datetime(timecols).toordinal())\n",
    "ipdata[numcols] = ipdata[numcols].astype(float)\n",
    "ipdata[catcols] = ipdata[catcols].astype('category')\n",
    "#ipdata.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode \n",
    "df = pd.get_dummies(df)\n",
    "df = df.drop(columns='PotentialFraud_No')\n",
    "\n",
    "# Logistic regression:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.drop(columns='PotentialFraud_Yes')\n",
    "Y = df[\"PotentialFraud_Yes\"]\n",
    "\n",
    "#df.info()\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "fit = logreg.fit(X, Y)\n",
    "\n",
    "fit.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "#.0001 = .333\n",
    "#.001 = .349\n",
    "#.01 = .639\n",
    "#.1 = 22.56\n",
    "#.2 = 78.74\n",
    "\n",
    "#df = ipdata\n",
    "df = ipdata.sample(frac=0.5, replace=False, random_state=1)\n",
    "df = pd.get_dummies(df)\n",
    "df = df.drop(columns='PotentialFraud_No')\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.drop(columns='PotentialFraud_Yes')\n",
    "Y = df[\"PotentialFraud_Yes\"]\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,Y)\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "#z = feat_importances.nlargest(10)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "hm = df.loc[:, numcols]\n",
    "#get correlations of each features in dataset\n",
    "corrmat = hm.corr()\n",
    "top_corr_features = corrmat.index\n",
    "\n",
    "top_corr_features\n",
    "plt.figure(figsize=(10,8))\n",
    "#plot heat map\n",
    "g=sns.heatmap(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt PCA:\n",
    "\n",
    "# Code from: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "features = df.columns\n",
    "# Separating out the features\n",
    "X = df.loc[:, features].values# Separating out the target\n",
    "Y = df.loc[:,['PotentialFraud_Yes']].values# Standardizing the features\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "finalDf = pd.concat([principalDf, df[['PotentialFraud_Yes']]], axis = 1)\n",
    "\n",
    "df[['PotentialFraud_Yes']]\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['PotentialFraud_Yes','PotentialFraud_No']\n",
    "colors = ['r', 'b']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['PotentialFraud_Yes'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ipdata[['ClmProcedureCode_1', 'DiagnosisGroupCode']]\n",
    "df = df[df['ClmProcedureCode_1'].isna() ]\n",
    "\n",
    "df[df['DiagnosisGroupCode'].isna() ]\n",
    "\n",
    "\n",
    "\n",
    "#df[df['DiagnosisGroupCode'] != df['DiagnosisGroupCode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge ICD-9 Procedure Code data with inpatient data to find missing codes:\n",
    "ipdata = ipdata.merge(proc_codes, left_on='ClmProcedureCode_1', right_on='PROCEDURE CODE', how='left')\n",
    "ipdata = ipdata.rename(columns={\"PROCEDURE CODE\": \"ProcedureCode_1\", \"SHORT DESCRIPTION\":\"ProcedureShortDesc_1\", \"LONG DESCRIPTION\": \"ProcedureLongDesc_1\"})\n",
    "ipdata = ipdata.merge(proc_codes, left_on='ClmProcedureCode_2', right_on='PROCEDURE CODE', how='left')\n",
    "ipdata = ipdata.rename(columns={\"PROCEDURE CODE\": \"ProcedureCode_2\", \"SHORT DESCRIPTION\":\"ProcedureShortDesc_2\", \"LONG DESCRIPTION\": \"ProcedureLongDesc_2\"})\n",
    "ipdata = ipdata.merge(proc_codes, left_on='ClmProcedureCode_3', right_on='PROCEDURE CODE', how='left')\n",
    "ipdata = ipdata.rename(columns={\"PROCEDURE CODE\": \"ProcedureCode_3\", \"SHORT DESCRIPTION\":\"ProcedureShortDesc_3\", \"LONG DESCRIPTION\": \"ProcedureLongDesc_3\"})\n",
    "ipdata = ipdata.merge(proc_codes, left_on='ClmProcedureCode_4', right_on='PROCEDURE CODE', how='left')\n",
    "ipdata = ipdata.rename(columns={\"PROCEDURE CODE\": \"ProcedureCode_4\", \"SHORT DESCRIPTION\":\"ProcedureShortDesc_4\", \"LONG DESCRIPTION\": \"ProcedureLongDesc_4\"})\n",
    "ipdata = ipdata.merge(proc_codes, left_on='ClmProcedureCode_5', right_on='PROCEDURE CODE', how='left')\n",
    "ipdata = ipdata.rename(columns={\"PROCEDURE CODE\": \"ProcedureCode_5\", \"SHORT DESCRIPTION\":\"ProcedureShortDesc_5\", \"LONG DESCRIPTION\": \"ProcedureLongDesc_5\"})\n",
    "\n",
    "# Merge ICD-9 Diagnosis Code data with inpatient data to find missing codes:\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_1', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_1\", \"DESCRIPTION\": \"DiagnosticDesc_1\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_2', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_2\", \"DESCRIPTION\": \"DiagnosticDesc_2\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_3', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_3\", \"DESCRIPTION\": \"DiagnosticDesc_3\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_4', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_4\", \"DESCRIPTION\": \"DiagnosticDesc_4\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_5', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_5\", \"DESCRIPTION\": \"DiagnosticDesc_5\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_6', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_6\", \"DESCRIPTION\": \"DiagnosticDesc_6\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_7', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_7\", \"DESCRIPTION\": \"DiagnosticDesc_7\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_8', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_8\", \"DESCRIPTION\": \"DiagnosticDesc_8\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_9', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_9\", \"DESCRIPTION\": \"DiagnosticDesc_9\"})\n",
    "ipdata = ipdata.merge(diag_codes, left_on='ClmDiagnosisCode_10', right_on='DIAGC', how='left')\n",
    "ipdata = ipdata.rename(columns={\"DIAGC\": \"DiagnosticCode_10\", \"DESCRIPTION\": \"DiagnosticDesc_10\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipdata.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipdata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Joint Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Attributes and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features\n",
    "Column|Description|Calculation\n",
    ":---|:---|:---\n",
    "DaysIP | Inpatient Days Stayed | DischargeDt - AdmissionDt + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipdata['DaysIP'] = ipdata['DischargeDt'] - ipdata['AdmissionDt'] + pd.Timedelta(1, unit='days')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known Fraud Cases: (TODO: add a dictionary or something here with various known scenarios?)\n",
    "\n",
    "## Number of claims with admission date after death:\n",
    "((ipdata['AdmissionDt'] - ipdata['DOD']) > pd.Timedelta(1, unit='days')).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rubric:\n",
    "## Business Understanding \t10\t\n",
    "*Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). Describe how you would define and measure the outcomes from the dataset. That is, why is this data important and how do you know if you have mined useful knowledge from the dataset? How would you measure the effectiveness of a good prediction algorithm? Be specific.* \n",
    "\n",
    "Class notes:  \n",
    "- First couple of sentences are straightforward.  MAKE SURE you answer why it's important (why we picked it) what are we going to predict using this dataset? \n",
    "- Once you picked it. It will be either continuous or categorical. YOU MUST also identify the algorithm.  I will measure effectiveness of the algorithm using _____RMSE or whatever______ ? \n",
    "- Continuous: regression problem RMSE, MAEP (mean absolute error percentage), R^2 (more for CV) \n",
    "- Categorical: classification problem.  Could use: accuracy, precision, recall, F1, ROC curve \n",
    "- I want to predict X, type of problem, measure effectiveness using ___ cross validation, and metric (precision/recall) \n",
    "- Mean absolute error: can predict within X percent of actual graduation rate \n",
    "- RMSE: pretty close to MAE, but if model gets something really wrong (true: 90%, predict 70%), RMSE penalizes really heavily for those big misses \n",
    "\n",
    "## Data Meaning Type \t10\t\n",
    "*Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file.*\n",
    "\n",
    "Class notes:\n",
    "- Dataframe.info() in pandas.  Can put it in a table and provide definitions (business info) \n",
    "\n",
    "## Data Quality\t15\t\n",
    "Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justifications for your methods.\n",
    "\n",
    "Class notes:\n",
    "- Hit all 6 things for full credit.  Code to check for each.  Boxplots. (don't need a boxplot for all 1000 fields) programmatic effort is ok for large # of fields.  Looking for explanations for why we decided to do something a certain way.  As long as it is explained well, should be full credit unless it is egregious.   \n",
    "\n",
    "## Simple Statistics\t10\t\n",
    "Visualize appropriate statistics (e.g., range, mode, mean, median, variance, counts) for a subset of attributes. Describe anything meaningful you found from this or if you found something potentially interesting. Note: You can also use data from other sources for comparison. Explain why the statistics run are meaningful. \n",
    "\n",
    "Class notes:\n",
    "- Dataframe.describe() and pick a couple of interesting fields and describe why interesting \n",
    "- What percent belongs to positive vs negative response variable \n",
    "\n",
    "## Visualize Attributes\t15\t\n",
    "Visualize the most interesting attributes (at least 5 attributes, your opinion on what is interesting). Important: Interpret the implications for each visualization. Explain for each attribute why the chosen visualization is appropriate.\n",
    "\n",
    "Class notes:\n",
    "- Two requirements for each viz you make that need to be written about \n",
    "- Need more than 1 sentence.  If take time to make viz, explain why we did it, and why it's significant to report \n",
    "- Report needs to be equal parts code & writing \n",
    "\n",
    "## Explore Joint Attributes\t15\t\n",
    "Visualize relationships between attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships.\n",
    "\n",
    "Class notes:\n",
    "- Viz two or more and analyze why they're different \n",
    "- Pair plot, or scatterplot for pairwise combos of features \n",
    "- Correlation heatmap, talk about highly correlated fields \n",
    "- If correlated w/ response woot, if another feature, may need cleanup \n",
    "\n",
    "## Explore Attributes and Class\t10\t\n",
    "Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification).\n",
    "\n",
    "Class notes:\n",
    "- Class = thing in dataset you're trying to predict \n",
    "- Ds.data and ds.target (target = response, data = features) \n",
    "- Can use visualizations here, Should tie together w/ visualizations  \n",
    "\n",
    "## New Features\t5\t\n",
    "Are there other features that could be added to the data or created from existing features? Which ones?\n",
    "\n",
    "Class notes:\n",
    "- Feature engineering, don't have to build them, just think about it and comment on what you might do \n",
    "- ML models are really basic, don't see the same things as people see \n",
    "- Can bin age/income, or create percentages \n",
    "- More info you can tell model, the better \n",
    "- If you're going to get some features from somewhere else, you must be explicit:\n",
    "- I.e. I want to add weather info: where is the data coming from, prove it exists \n",
    "- Could be a couple of bullet points with what we're going to create \n",
    "\n",
    "## Exceptional Work\t10\t\n",
    "You have free reign to provide additional analyses. One idea: implement dimensionality reduction, then visualize and interpret the results.\n",
    "\n",
    "Class notes:\n",
    "- Can pick what you want to work on. PCA. Scree plot, talk about it.  Be creative or smart.  Could build a model (log regression or SVM) for exceptional points, and then cut/paste into lab 2 \n",
    "- Here are the things I'd like to be considered for exceptional points: if you do code and explain it, should be point worthy.  Needs to be substantial.  I.e. make it good. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}