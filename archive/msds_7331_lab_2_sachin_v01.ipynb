{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6> <b> SMU Lab Two - MSDS7331 - Machine Learning-1 </b> </font>\n",
    "\n",
    "<font size=5> <b> Summer 2020 Group - Sachin, Ikenna, Edgar, Dustin </b></font> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/sachinac/ML7331/blob/master/data/data_mining.jpg?raw=true\"> \n",
    "\n",
    "<p align=\"center\"><font size=5> <b> Health Care Fraud Detection  </b></font> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\">Import Libraries</a></span></li><li><span><a href=\"#Data-Preparation-Part-1\" data-toc-modified-id=\"Data-Preparation-Part-1-2\">Data Preparation Part 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2.1\">Load Data</a></span></li><li><span><a href=\"#Set-Categorical-Variables\" data-toc-modified-id=\"Set-Categorical-Variables-2.2\">Set Categorical Variables</a></span></li><li><span><a href=\"#Set-Float-Variables\" data-toc-modified-id=\"Set-Float-Variables-2.3\">Set Float Variables</a></span></li><li><span><a href=\"#Set-Integer-Variables\" data-toc-modified-id=\"Set-Integer-Variables-2.4\">Set Integer Variables</a></span></li><li><span><a href=\"#Set-Date-Variables\" data-toc-modified-id=\"Set-Date-Variables-2.5\">Set Date Variables</a></span></li><li><span><a href=\"#Recoding-Binary-and-Categorical-Features\" data-toc-modified-id=\"Recoding-Binary-and-Categorical-Features-2.6\">Recoding Binary and Categorical Features</a></span></li><li><span><a href=\"#Final-Dataframe---All-Features\" data-toc-modified-id=\"Final-Dataframe---All-Features-2.7\">Final Dataframe - All Features</a></span></li><li><span><a href=\"#Set-Target-Variable\" data-toc-modified-id=\"Set-Target-Variable-2.8\">Set Target Variable</a></span></li><li><span><a href=\"#One-Hot-Encoding-using-SciKit-Learn-Multi-Label-Binarizer\" data-toc-modified-id=\"One-Hot-Encoding-using-SciKit-Learn-Multi-Label-Binarizer-2.9\">One Hot Encoding using SciKit Learn Multi Label Binarizer</a></span></li><li><span><a href=\"#One-Hot-Encoding-using-Pandas\" data-toc-modified-id=\"One-Hot-Encoding-using-Pandas-2.10\">One Hot Encoding using Pandas</a></span></li><li><span><a href=\"#Post-processing-Encoded-Features\" data-toc-modified-id=\"Post-processing-Encoded-Features-2.11\">Post-processing Encoded Features</a></span></li><li><span><a href=\"#Remove-'None'-columns\" data-toc-modified-id=\"Remove-'None'-columns-2.12\">Remove 'None' columns</a></span></li><li><span><a href=\"#MinMaxScaler\" data-toc-modified-id=\"MinMaxScaler-2.13\">MinMaxScaler</a></span></li><li><span><a href=\"#Sparse-Matrix-Conversion-and-Test/Train-Split\" data-toc-modified-id=\"Sparse-Matrix-Conversion-and-Test/Train-Split-2.14\">Sparse Matrix Conversion and Test/Train Split</a></span></li></ul></li><li><span><a href=\"#Data-Preparation-Part-2\" data-toc-modified-id=\"Data-Preparation-Part-2-3\">Data Preparation Part 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Final-DataSet\" data-toc-modified-id=\"Final-DataSet-3.1\">Final DataSet</a></span></li></ul></li><li><span><a href=\"#Modeling-and-Evaluations-1\" data-toc-modified-id=\"Modeling-and-Evaluations-1-4\">Modeling and Evaluations 1</a></span></li><li><span><a href=\"#Modeling-and-Evaluations-2\" data-toc-modified-id=\"Modeling-and-Evaluations-2-5\">Modeling and Evaluations 2</a></span></li><li><span><a href=\"#Modeling-and-Evaluations-3\" data-toc-modified-id=\"Modeling-and-Evaluations-3-6\">Modeling and Evaluations 3</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Models\" data-toc-modified-id=\"Create-Models-6.1\">Create Models</a></span></li></ul></li><li><span><a href=\"#Modeling-and-Evaluations-4\" data-toc-modified-id=\"Modeling-and-Evaluations-4-7\">Modeling and Evaluations 4</a></span></li><li><span><a href=\"#Modeling-and-Evaluations-5\" data-toc-modified-id=\"Modeling-and-Evaluations-5-8\">Modeling and Evaluations 5</a></span></li><li><span><a href=\"#Modeling-and-Evaluations-6\" data-toc-modified-id=\"Modeling-and-Evaluations-6-9\">Modeling and Evaluations 6</a></span></li><li><span><a href=\"#Deployment\" data-toc-modified-id=\"Deployment-10\">Deployment</a></span></li><li><span><a href=\"#Exceptional-Work\" data-toc-modified-id=\"Exceptional-Work-11\">Exceptional Work</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing:\" data-toc-modified-id=\"Preprocessing:-11.0.1\">Preprocessing:</a></span></li><li><span><a href=\"#Pipelines-and-Grid-Search:\" data-toc-modified-id=\"Pipelines-and-Grid-Search:-11.0.2\">Pipelines and Grid Search:</a></span></li></ul></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-12\">References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:23.614360Z",
     "start_time": "2020-07-04T15:14:22.497921Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import timeit\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import csc_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data into memory. EDA was already performed on this data. We had originally received Beneficiary, Encounters and target variable datasets in three different spreadsheets and EDA combined that into single spreadsheet using keys of the tables. We still need to perform some additional operations before we actually start with modeling. So in first we prepare our data for modeling as follows :\n",
    "* Prepare variables. Setup data type correctly.\n",
    "* Remove unnecessary variables\n",
    "* Transform categorical features into dummy variables \n",
    "* Feature selection.\n",
    "\n",
    "Lets first print the information of this dataframe. As we can see from below dataframe info this data has total of 79 features. We definately dont need all features. This step will process some of the features before we actually use this data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:24.186319Z",
     "start_time": "2020-07-04T15:14:23.691766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lab2_df = pd.read_csv('data/final_fraud_dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T08:51:44.322459Z",
     "start_time": "2020-06-30T08:51:44.318865Z"
    }
   },
   "source": [
    "Following are nominal categorical attributes. Pandas requires these to be datatype of 'object' or 'category'. We are setting nominal categorical variables as 'object'. Here is the list of categorical variables :\n",
    "\n",
    "* Race\n",
    "* Gender\n",
    "* RenalDiseaseIndicator\n",
    "* State\n",
    "* County\n",
    "* AttendingPhysicianPresent\n",
    "* OtherPhysicianPresent\n",
    "* OperatingPhysicianPresent\n",
    "* 11 Chronic Conditions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:24.797627Z",
     "start_time": "2020-07-04T15:14:24.551999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_preds = ['Gender','Race','RenalDiseaseIndicator','State','County',\n",
    "                     'NoOfMonths_PartACov','NoOfMonths_PartBCov',\n",
    "                     'ChronicCond_Alzheimer','ChronicCond_Heartfailure',\n",
    "                     'ChronicCond_KidneyDisease','ChronicCond_Cancer','ChronicCond_ObstrPulmonary',\n",
    "                     'ChronicCond_Depression','ChronicCond_Diabetes','ChronicCond_IschemicHeart',\n",
    "                     'ChronicCond_Osteoporasis','ChronicCond_rheumatoidarthritis','ChronicCond_stroke',\n",
    "                     'Alive','ClmAdmitDiagnosisCode','DiagnosisGroupCode',\n",
    "                     'ClmDiagnosisCode_1','ClmDiagnosisCode_2','ClmDiagnosisCode_3',\n",
    "                     'ClmDiagnosisCode_4','ClmDiagnosisCode_5','ClmDiagnosisCode_6',\n",
    "                     'ClmDiagnosisCode_7','ClmDiagnosisCode_8','ClmDiagnosisCode_9',\n",
    "                     'ClmDiagnosisCode_10','ClmProcedureCode_1','ClmProcedureCode_2',\n",
    "                     'ClmProcedureCode_3','ClmProcedureCode_4','ClmProcedureCode_5',\n",
    "                     'AttendingPhysicianPresent','OtherPhysicianPresent','OperatingPhysicianPresent']\n",
    "\n",
    "\n",
    "lab2_df[cat_preds]   = lab2_df[cat_preds].astype('object')\n",
    "\n",
    "\n",
    "lab2_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Float Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following varibles are required to be datatype of floating point. These are amounts and hence it makes sense to changt it's type to float\n",
    "\n",
    "* InscClaimAmtReimbursed\n",
    "* IPAnnualReimbursementAmt\n",
    "* IPAnnualDeductibleAmt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:25.385814Z",
     "start_time": "2020-07-04T15:14:25.376649Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "float_preds = ['InscClaimAmtReimbursed','IPAnnualReimbursementAmt', 'IPAnnualDeductibleAmt']\n",
    "\n",
    "lab2_df[float_preds]   = lab2_df[float_preds].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Integer Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables contains all numbers but they are actually nominal categorical attributes. They are all binary in nature. So we can keep them as integer as we will be converting this dataset into sparse matrix for model building exercise. \n",
    "\n",
    "Following variables will be set as integer data type :\n",
    "* NoOfMonths_PartACov - Number of months of Medicare part A coverage\n",
    "* NoOfMonths_PartBCov - Number of months of Medicare part B coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:26.303918Z",
     "start_time": "2020-07-04T15:14:26.156742Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "int_preds = ['NoOfMonths_PartACov', \n",
    "             'NoOfMonths_PartBCov']\n",
    "\n",
    "lab2_df[int_preds] = lab2_df[int_preds].astype(int)\n",
    "lab2_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Date Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are new attributes are derived from existing attritbutes of type dates. All date attributes are converted to the proleptic Gregorian ordinal of a date.In simple terms datetime.toordinal() returns the day count from the date 01/01/01\n",
    "\n",
    "\n",
    "* ORD_DOD - Set ORD_DOD to open end date where date is not available to indicate that Beneficiary is alive\n",
    "* ORD_DOB \n",
    "* ORD_ClaimStartDt\n",
    "* ORD_ClaimEndDt\n",
    "* ORD_AdmissionDt\n",
    "* ORD_DischargeDt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:28.080719Z",
     "start_time": "2020-07-04T15:14:27.016891Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dte_preds = ['ClaimStartDt', 'ClaimEndDt','AdmissionDt', 'DischargeDt', 'DOB','DOD']\n",
    "\n",
    "# Regex to normalize integer DOB/DOD as ISO dates\n",
    "lab2_df['ORD_DOB'] = lab2_df['DOB'].astype(str).str.replace('(\\d{4})(\\d\\d)(\\d\\d)', '\\\\1-\\\\2-\\\\3', regex=True) \n",
    "lab2_df['ORD_DOD'] = lab2_df['DOD'].astype(str).str.replace('(\\d{4})(\\d\\d)(\\d\\d)', '\\\\1-\\\\2-\\\\3', regex=True) \n",
    "\n",
    "lab2_df['ORD_DOD'] = lab2_df['DOD']\n",
    "lab2_df['ORD_DOD'] = lab2_df['ORD_DOD'].replace('0','2199-12-31')\n",
    "\n",
    "lab2_df['ORD_DOD'] = pd.to_datetime(lab2_df['ORD_DOD'],format='%Y-%m-%d').apply(dt.datetime.toordinal)\n",
    "lab2_df['ORD_DOB'] = pd.to_datetime(lab2_df['DOB'],format='%Y-%m-%d').apply(dt.datetime.toordinal)\n",
    "lab2_df['ORD_ClaimStartDt'] = pd.to_datetime(lab2_df['ClaimStartDt'],format='%Y-%m-%d').apply(dt.datetime.toordinal)\n",
    "lab2_df['ORD_ClaimEndDt']   = pd.to_datetime(lab2_df['ClaimEndDt'],format='%Y-%m-%d').apply(dt.datetime.toordinal)\n",
    "lab2_df['ORD_AdmissionDt']  = pd.to_datetime(lab2_df['AdmissionDt'],format='%Y-%m-%d').apply(dt.datetime.toordinal)\n",
    "lab2_df['ORD_DischargeDt']  = pd.to_datetime(lab2_df['DischargeDt'],format='%Y-%m-%d').apply(dt.datetime.toordinal)\n",
    "\n",
    "ord_dte_preds = ['ORD_DOD', 'ORD_DOB','ORD_ClaimStartDt', 'ORD_ClaimEndDt', 'ORD_AdmissionDt','ORD_DischargeDt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:28.593763Z",
     "start_time": "2020-07-04T15:14:28.484037Z"
    }
   },
   "outputs": [],
   "source": [
    "lab2_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recoding Binary and Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All chronic conditions, Gender, RenalDiseaseIndicator are binary variables i.e. they all have just two values but values are not 0 and 1. e.g. Gender has values 1 or 2, RenalDiseaseIndicator has Y or 1 and all Chronic Conditions has values 1 or 2. \n",
    "\n",
    "We will recode these values to 0 or 1 instead of 1 and 2 for modeling purpose. Leaving two columns for a binary feature can introduce bias, giving one feature artificially more weight in prediction if the model treats the single feature as two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T15:14:29.872412Z",
     "start_time": "2020-07-04T15:14:28.976765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recode below categorical variables \n",
    "\n",
    "lab2_df['Gender']  = lab2_df['Gender'].replace([1,2],[0,1])\n",
    "lab2_df['RenalDiseaseIndicator']  = lab2_df['RenalDiseaseIndicator'].replace('Y',1)\n",
    "\n",
    "\n",
    "lab2_df = lab2_df.replace({'ChronicCond_Alzheimer': 2,      'ChronicCond_Heartfailure': 2, \n",
    "                           'ChronicCond_KidneyDisease': 2,  'ChronicCond_Cancer': 2, \n",
    "                           'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2, \n",
    "                           'ChronicCond_Diabetes': 2,       'ChronicCond_IschemicHeart': 2, \n",
    "                           'ChronicCond_Osteoporasis': 2,   'ChronicCond_rheumatoidarthritis': 2, \n",
    "                           'ChronicCond_stroke': 2 }, '0')\n",
    "\n",
    "\n",
    "lab2_df['Race'] = lab2_df['Race'].astype('object')\n",
    "lab2_df['State'] = lab2_df['State'].astype('object')\n",
    "lab2_df['County'] = lab2_df['County'].astype('object')\n",
    "lab2_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataframe - All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are GROUPED as follows:\n",
    "* Numeric Predictos\n",
    "* Categorical Predictors\n",
    "* Non-Predictors ( Will not be used for modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T14:27:37.165216Z",
     "start_time": "2020-07-04T14:27:37.132411Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_preds     = ['Age','NoPhysician','NoOfDiag','NoOfProc',\n",
    "                     'ORD_DOD','ORD_DOB','ORD_ClaimStartDt','ORD_ClaimEndDt',\n",
    "                     'ORD_AdmissionDt','ORD_DischargeDt','DaysAdmitted',\n",
    "                     'NoOfMonths_PartACov','NoOfMonths_PartBCov',\n",
    "                     'IPAnnualReimbursementAmt','IPAnnualDeductibleAmt',   \n",
    "                     'InscClaimAmtReimbursed'  \n",
    "                    ]\n",
    "\n",
    "cat_preds_nominal = ['Gender','Race','RenalDiseaseIndicator','State','County',\n",
    "                     'ChronicCond_Alzheimer','ChronicCond_Heartfailure',\n",
    "                     'ChronicCond_KidneyDisease','ChronicCond_Cancer','ChronicCond_ObstrPulmonary',\n",
    "                     'ChronicCond_Depression','ChronicCond_Diabetes','ChronicCond_IschemicHeart',\n",
    "                     'ChronicCond_Osteoporasis','ChronicCond_rheumatoidarthritis','ChronicCond_stroke',\n",
    "                     'Alive','ClmAdmitDiagnosisCode','DiagnosisGroupCode',\n",
    "                     'ClmDiagnosisCode_1','ClmDiagnosisCode_2','ClmDiagnosisCode_3',\n",
    "                     'ClmDiagnosisCode_4','ClmDiagnosisCode_5','ClmDiagnosisCode_6',\n",
    "                     'ClmDiagnosisCode_7','ClmDiagnosisCode_8','ClmDiagnosisCode_9',\n",
    "                     'ClmDiagnosisCode_10','ClmProcedureCode_1','ClmProcedureCode_2',\n",
    "                     'ClmProcedureCode_3','ClmProcedureCode_4','ClmProcedureCode_5',\n",
    "                     'AttendingPhysicianPresent','OtherPhysicianPresent','OperatingPhysicianPresent'\n",
    "                     ]\n",
    "\n",
    "non_preds = ['BeneID','DOB','DOD','state_usps','ClaimID',\n",
    "             'ClaimStartDt','ClaimEndDt','Provider',\n",
    "             'AttendingPhysician','OperatingPhysician','OtherPhysician',\n",
    "             'AdmissionDt','DischargeDt','DRGDesc',\n",
    "             'ProcedureShortDesc_1','ProcedureShortDesc_2','ProcedureShortDesc_3',\n",
    "             'ProcedureShortDesc_4','ProcedureShortDesc_5','DiagnosticDesc_1',\n",
    "             'DiagnosticDesc_2','DiagnosticDesc_3','DiagnosticDesc_4',\n",
    "             'DiagnosticDesc_5','DiagnosticDesc_6','DiagnosticDesc_7',\n",
    "             'DiagnosticDesc_8','DiagnosticDesc_9','DiagnosticDesc_10',\n",
    "             'NoOfChronicCondition'\n",
    "             ]\n",
    "\n",
    "preds_in_model = numeric_preds + cat_preds_nominal\n",
    "\n",
    "print(preds_in_model)\n",
    "\n",
    "Xlab2_df = lab2_df[preds_in_model]\n",
    "\n",
    "lab2_df.Gender.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T13:44:22.720692Z",
     "start_time": "2020-07-04T13:44:22.706713Z"
    }
   },
   "outputs": [],
   "source": [
    "target_df = lab2_df['PotentialFraud'].replace(['Yes','No'],[1,0]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding using SciKit Learn Multi Label Binarizer\n",
    "\n",
    "Because the claim data can have up to 10 different diagnosis codes and 5 different procedure codes, we created a new array column that combines all used diagnosis codes and another like type column with procedures.  This will reduce our feature counts from >40,000 to around 9,000 and will help maximize feature importance because each code will no longer be split across up to 10 different columns (i.e. diagnosis code columns 1 - 10).  This way if code A gets used in column_1, and in column_2, the usage of code A will be consolidated to the DiagnosisCode_A column.\n",
    "\n",
    "Pandas get_dummies function will not parse lists, we'll need to utilize the multiLabelBinarizer from scikit learn to one-hot encode the list values before calling get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T13:44:34.972441Z",
     "start_time": "2020-07-04T13:44:23.450784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Xlab2_df['DiagnosisCode'] = Xlab2_df[['ClmDiagnosisCode_1', 'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10']].values.tolist()\n",
    "\n",
    "Xlab2_df['ProcedureCode'] = Xlab2_df[['ClmProcedureCode_1', 'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4', 'ClmProcedureCode_5']].values.tolist()\n",
    "\n",
    "ipdata = Xlab2_df.copy()\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "ipdata = ipdata.join(pd.DataFrame(mlb.fit_transform(ipdata['ProcedureCode']),columns='ProcedureCode_'+mlb.classes_))\n",
    "ipdata = ipdata.join(pd.DataFrame(mlb.fit_transform(ipdata['DiagnosisCode']),columns='DiagnosisCode_'+mlb.classes_))\n",
    "\n",
    "ipdata = ipdata.drop(columns=['ClmProcedureCode_1', 'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4', 'ClmProcedureCode_5','ClmDiagnosisCode_1', 'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10','ProcedureCode','DiagnosisCode'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding using Pandas\n",
    "\n",
    "Now that we've dropped our multi-columns and lists, we can onehot encode the rest of the features and transform into a sparse matrix using Pandas Get Dummies function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:25:36.420936Z",
     "start_time": "2020-07-03T22:25:31.646582Z"
    }
   },
   "outputs": [],
   "source": [
    "ipdata = pd.get_dummies(ipdata, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:25:39.929129Z",
     "start_time": "2020-07-03T22:25:36.982844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing Encoded Features\n",
    "\n",
    "Because we are using objects, Pandas converts all classes into new columns.  However, for binary features, we want to remove one of the values, in order to prevent the model from introducing bias by weighting each binary feature twice (via separate true/false columns).\n",
    "\n",
    "To correct this, we must search for and remove duplicate columns (indicated by the _0 suffix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:27:20.105748Z",
     "start_time": "2020-07-03T22:27:17.996438Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "StopWords = ['Diagnosis','Procedure','County','State']\n",
    "bins = []\n",
    "print('Removed Following Columns:')\n",
    "for col in ipdata.columns:\n",
    "    if not any(word in col for word in StopWords) and '_0' in col:    \n",
    "        print(col)\n",
    "        bins.append(col)\n",
    "ipdata = ipdata.drop(bins, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove 'None' columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our encoded dataset contains None for Diagnosis and Procedure codes features wherever information is not available on the claim. All claims allows max 5 procedure codes and 10 diagnosis codes and at least one procedure code or diagnosis code is applied on the claim. When claim has only one procedure and one diagnosis all other procedure codes contains NAs or None and one hot encoding treats these as another category which is not correct. therefore we need to remove column Nones created by one hot encoding by pandas.\n",
    "\n",
    "Essentially by creating a 'none' column it creates a column (new feature) for missing data, which would erroneously assign more weight to a value that doesn't exist.  Because it would get a '1' for missing a value, but would also have 0's for the other values.  Also it may mess up our counts because 'None' would count as a valid procedure code if we're summing unique values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:27:22.370338Z",
     "start_time": "2020-07-03T22:27:20.739584Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nones = []\n",
    "print('Removed Following Columns:')\n",
    "for col in ipdata.columns:\n",
    "    if 'None' in col:    \n",
    "        nones.append(col)\n",
    "        print(col)\n",
    "ipdata = ipdata.drop(nones, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:27:30.379173Z",
     "start_time": "2020-07-03T22:27:30.274229Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ipdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the MinMaxScaler here instead of the standard scaler because we're using a sparse matrix of mostly binary features.  Because we do have some true numeric features, such as: deductible amount, age, the converted ordinal dates, claim dollar amounts etc, these may end up having values outside the bounds of 0 and 1.  A zero mean scaler doesn't make sense with this data because it would cause the numeric features to have inflated importance if observations end up being several standard deviations away from 0, while the binary features are capped at 1.  The minMax scaler allows us to force all values in the dataset to live within the bounds of 0 and 1, allowing our logistic regression algorithms to weight features appropriately using similar measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:32:53.236460Z",
     "start_time": "2020-07-03T22:32:04.475365Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(ipdata)\n",
    "X1 = scaler.transform(ipdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrix Conversion and Test/Train Split\n",
    "The minMax Scaler returns a dense numpy array.  Because we have so many features, we need to transform the data back into a sparse matrix for efficient . **CSC(Compressed Sparse Column)** is more efficient at accessing column-vectors or column operations, generally, as it is stored as arrays of columns and their value at each row.\n",
    "\n",
    "We chose to scale, convert to sparse and do test/train splits all before our pipeline in order to save processing time in the pipeline and grid search model execution steps.  The scaler takes a few minutes, and we don't want to introduce this lag to the repetitive pipeline.  This also gives us the advantage of using the exact same test/train splits for all models for a fair evaluation.  10 fold cross validation will utilize it's own splits of the training data, and this ensures we don't overfit to the training data, in addition to witholding test data entirely from the models so we can evaluation generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:33:11.178163Z",
     "start_time": "2020-07-03T22:33:03.584212Z"
    }
   },
   "outputs": [],
   "source": [
    "X = csc_matrix(X1)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,target_df,test_size=0.2,random_state=86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 2\n",
    "\n",
    "We ended up with two variants of our final dataset: the sparse matrix (ipdata) containing all one-hot encoded features, and the original dataframe (Xlab2_df) so that if we need to retrieve any of the original data, we can share indices to highlight our findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:22:57.784631Z",
     "start_time": "2020-07-01T06:22:57.649511Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xlab2_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:22:57.872722Z",
     "start_time": "2020-07-01T06:22:57.786692Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:22:58.026733Z",
     "start_time": "2020-07-01T06:22:57.879730Z"
    }
   },
   "outputs": [],
   "source": [
    "Xlab2_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluations 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluations 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluations 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T06:48:18.365325Z",
     "start_time": "2020-07-01T06:48:18.361523Z"
    }
   },
   "source": [
    "<b>Classification Models </b>\n",
    "* Logistic regression\n",
    "* Random Forest\n",
    "* Naive Bayes\n",
    "\n",
    "<b>Regression Models </b>\n",
    "* Multiple Linear Regression\n",
    "* Random Forest\n",
    "* kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:42:26.639880Z",
     "start_time": "2020-07-03T22:41:48.201821Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"This is copy/pasted from mini-lab.  Will need to refactor into pipe/grid search\"\"\"\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# fit the model\n",
    "logreg = LogisticRegression(penalty='l1', random_state=86, solver='liblinear')\n",
    "\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "fit_time = timeit.default_timer() - start_time\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=.2, random_state=86)\n",
    "f1scores = cross_val_score(logreg, X_train, y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
    "PreScores = cross_val_score(logreg, X_train, y_train, cv=cv, scoring='precision', n_jobs=-1)\n",
    "RecScores = cross_val_score(logreg, X_train, y_train, cv=cv, scoring='recall', n_jobs=-1)\n",
    "print('F1 scores:', f1scores)\n",
    "print('\\nPrecision scores', PreScores)\n",
    "print('\\nRecall scores', RecScores)\n",
    "\n",
    "print('\\nAverage F1: ',np.average(f1scores))\n",
    "print('Min F1: ',np.min(f1scores))\n",
    "print('Max F1: ',np.max(f1scores))\n",
    "print('\\nAverage Precision: ',np.average(PreScores))\n",
    "print('Min Precision: ',np.min(PreScores))\n",
    "print('Max Precision: ',np.max(PreScores))\n",
    "print('\\nAverage Recall: ',np.average(RecScores))\n",
    "print('Min Recall: ',np.min(RecScores))\n",
    "print('Max Recall: ',np.max(RecScores))\n",
    "\n",
    "\n",
    "cv_time = timeit.default_timer() - start_time - fit_time\n",
    "print(\"\\nFit time: \", fit_time)\n",
    "print(\"CV time: \", cv_time)\n",
    "print(\"Total time: \", timeit.default_timer() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize feature importance\n",
    "zip_vars = zip(ipdata.columns,logreg.coef_.T) # combine attributes\n",
    "FeatureWeights = pd.DataFrame(list(zip_vars), columns=['Feature', 'Weight'])\n",
    "\n",
    "FeatureWeights.sort_values(by='Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluations 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T22:44:17.286956Z",
     "start_time": "2020-07-03T22:44:17.281744Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T05:46:33.779286Z",
     "start_time": "2020-07-04T05:46:33.772487Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "#pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "scoring = {'AUC': 'roc_auc', 'Recall': make_scorer(accuracy_score),'Precision': make_scorer(accuracy_score)}\n",
    "scores = ['precision', 'recall']\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    #'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__C' : np.logspace(-4, 4),\n",
    "    'classifier__solver' : ['liblinear']},\n",
    "     {'classifier' : [RandomForestClassifier()],\n",
    "    'classifier__n_estimators' : list(range(10,101,10)),\n",
    "     'classifier__n_estimators' : list(range(10,101)),\n",
    "    #'classifier__max_features' : list(range(6,32,5))}\n",
    "     'classifier__max_features' : list(range(6,32))}\n",
    "]\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1,scoring=scoring,refit='AUC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T07:43:47.331486Z",
     "start_time": "2020-07-04T05:46:36.483174Z"
    }
   },
   "outputs": [],
   "source": [
    "best_clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T00:19:51.922164Z",
     "start_time": "2020-07-04T00:19:51.907389Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(best_clf.cv_results_).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T00:32:15.016696Z",
     "start_time": "2020-07-04T00:32:14.851471Z"
    }
   },
   "outputs": [],
   "source": [
    "results= pd.DataFrame.from_dict(best_clf.cv_results_)\n",
    "xrange=list(range(1,101))\n",
    "ax = sns.lineplot(x=list(range(1,101)), y=\"rank_test_Precision\", data=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:06:52.323405Z",
     "start_time": "2020-07-04T03:06:52.276604Z"
    }
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame({'obs': xrange, 'precision': results.rank_test_Precision})\n",
    "best_clf.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluations 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluations 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "### Preprocessing:\n",
    "We would like credit for the large amount of preprocessing that went into our data.  We have a large variety of features in the dataset, many of which required clever feature engineering tricks to improve performance.  Conversion to the sparse matrix took a good bit of research, and continues to be an effort for implementation, since not every model or library will accept a sparse matrix.  The conversion of procedure codes and diagnosis codes from multiple columns into a one hot encoded dataframe took a significant amount of time/effort, most of the out of the box libraries we tried would not parse a list object and it took a lot of experimentation and research.  The performance improved as it reduced our total featureset footprint from > 40,000 to around 9,000.   \n",
    "\n",
    "### Pipelines and Grid Search:\n",
    "We decided to implement Pipelines and Grid Search in order to reduce the copy/paste and number of variables we need to work with.  This is a new concept barely touched on in class, and it required significant amounts of research time to implement before we could move on to model evaluation, visualization and discussion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:12:34.194394Z",
     "start_time": "2020-07-04T03:12:34.166395Z"
    }
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'num': list(range(1,101)),\n",
    "    'A': results.split0_test_Recall,\n",
    "    'B': results.split1_test_Recall,\n",
    "    'C': results.split2_test_Recall,\n",
    "    'D': results.param_classifier__penalty,}\n",
    "               )\n",
    "\n",
    "\n",
    "alt.Chart(data).transform_fold(\n",
    "    ['A', 'B', 'C','D'],\n",
    ").mark_line().encode(\n",
    "    x='num:N',\n",
    "    y='value:Q',\n",
    "    color='D:N'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:09:38.730564Z",
     "start_time": "2020-07-04T03:09:38.592859Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.violinplot(x=results[\"mean_score_time_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:23:27.058076Z",
     "start_time": "2020-07-04T03:23:25.003138Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons()\n",
    "calibrated_forest = CalibratedClassifierCV(\n",
    "   base_estimator=RandomForestClassifier(n_estimators=10))\n",
    "param_grid = {\n",
    "   'base_estimator__max_depth': [2, 4, 6, 8]}\n",
    "search = GridSearchCV(calibrated_forest, param_grid, cv=5)\n",
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:23:45.489088Z",
     "start_time": "2020-07-04T03:23:41.974501Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "pipe = Pipeline([\n",
    "   ('select', SelectKBest()),\n",
    "   ('model', calibrated_forest)])\n",
    "param_grid = {\n",
    "   'select__k': [1, 2],\n",
    "   'model__base_estimator__max_depth': [2, 4, 6, 8]}\n",
    "search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:24:03.603975Z",
     "start_time": "2020-07-04T03:24:03.596319Z"
    }
   },
   "outputs": [],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T03:24:38.828863Z",
     "start_time": "2020-07-04T03:24:38.806896Z"
    }
   },
   "outputs": [],
   "source": [
    "cvrest= pd.DataFrame.from_dict(search.cv_results_)\n",
    "cvrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'num': list(range(1,101)),\n",
    "    'A': cvrest.split0_test_score,\n",
    "    'B': cvrest.split1_test_score,\n",
    "    'C': cvrest.split2_test_score,\n",
    "    'D': cvrest.param_classifier__penalty,}\n",
    "               )\n",
    "\n",
    "\n",
    "alt.Chart(data).transform_fold(\n",
    "    ['A', 'B', 'C','D'],\n",
    ").mark_line().encode(\n",
    "    x='num:N',\n",
    "    y='value:Q',\n",
    "    color='D:N'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T08:42:11.116408Z",
     "start_time": "2020-07-04T08:42:11.019987Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(Xlab2_df,target_df,test_size=0.2,random_state=86)\n",
    "X_train.loc[:,cat_preds_nominal].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T08:25:42.272120Z",
     "start_time": "2020-07-04T08:25:42.254709Z"
    }
   },
   "outputs": [],
   "source": [
    "target_df = lab2_df['PotentialFraud'].replace(['Yes','No'],[1,0])\n",
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T08:25:43.146775Z",
     "start_time": "2020-07-04T08:25:42.914433Z"
    }
   },
   "outputs": [],
   "source": [
    "from feature_engine.categorical_encoders import MeanCategoricalEncoder\n",
    "\n",
    "mean_enc = MeanCategoricalEncoder(\n",
    "    variables=cat_preds_nominal)\n",
    "mean_enc.fit(X_train.loc[:,cat_preds_nominal], target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T08:25:45.035533Z",
     "start_time": "2020-07-04T08:25:45.012600Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = mean_enc.transform(X_train.loc[:,cat_preds_nominal])\n",
    "X_test = mean_enc.transform(X_test.loc[:,cat_preds_nominal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T08:18:49.616530Z",
     "start_time": "2020-07-04T08:18:49.611385Z"
    }
   },
   "outputs": [],
   "source": [
    "Xlab2_df.Gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 365,
   "position": {
    "height": "387px",
    "left": "789px",
    "right": "20px",
    "top": "120px",
    "width": "630px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
